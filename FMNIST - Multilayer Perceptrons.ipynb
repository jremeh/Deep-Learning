{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is the work book following this tutorial, his explanation is good, and a relly good hands on for Keras. Weights and Bias also offers a good visualization on the results.\nhttps://github.com/lukas/ml-class\nhttps://www.youtube.com/watch?v=GVKDa5hxUZE&list=PLD80i8An1OEHSai9cf-Ip-QReOVW76PlB&index=4"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nimport wandb\nfrom wandb.keras import WandbCallback","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logging code\nrun = wandb.init(project=\"fashion\")","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n","name":"stderr"},{"output_type":"stream","name":"stdout","text":"API Key: ········\n"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/3dpdwtiy\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/3dpdwtiy</a><br/>\n            "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nlabels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\n          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = y_train.shape[1]\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\nmodel.add(Dense(num_classes))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":9,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.4702 - accuracy: 0.1520 - val_loss: 10.3542 - val_accuracy: 0.1008\nEpoch 2/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.4142 - accuracy: 0.1081 - val_loss: 9.9449 - val_accuracy: 0.1121\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.2545 - accuracy: 0.1253 - val_loss: 10.1077 - val_accuracy: 0.1347\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9531 - accuracy: 0.1927 - val_loss: 8.8005 - val_accuracy: 0.2047\nEpoch 5/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.7858 - accuracy: 0.2160 - val_loss: 10.0220 - val_accuracy: 0.2437\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.1406 - accuracy: 0.2353 - val_loss: 8.3360 - val_accuracy: 0.2115\nEpoch 7/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 8.8255 - accuracy: 0.2449 - val_loss: 8.8730 - val_accuracy: 0.2063\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 8.8564 - accuracy: 0.2393 - val_loss: 9.0681 - val_accuracy: 0.2484\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2431 - accuracy: 0.1037 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 10/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf6600f6d0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Result isn't that good, let's debug by trying a few changes:\n\n1. Add more training"},{"metadata":{"trusted":true},"cell_type":"code","source":"run = wandb.init(project=\"fashion\")\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\nmodel.add(Dense(num_classes))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/5xwfmv3y\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/5xwfmv3y</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.6887 - accuracy: 0.1954 - val_loss: 8.9005 - val_accuracy: 0.1978\nEpoch 2/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.8090 - accuracy: 0.1735 - val_loss: 11.3971 - val_accuracy: 0.1139\nEpoch 3/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.6617 - accuracy: 0.1155 - val_loss: 9.3163 - val_accuracy: 0.1159\nEpoch 4/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.7699 - accuracy: 0.1328 - val_loss: 10.6944 - val_accuracy: 0.1022\nEpoch 5/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.3756 - accuracy: 0.1162 - val_loss: 10.8330 - val_accuracy: 0.1831\nEpoch 6/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.5645 - accuracy: 0.1921 - val_loss: 8.1387 - val_accuracy: 0.2247\nEpoch 7/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.0414 - accuracy: 0.2054 - val_loss: 9.9626 - val_accuracy: 0.1864\nEpoch 8/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.0521 - accuracy: 0.1737 - val_loss: 10.1415 - val_accuracy: 0.1461\nEpoch 9/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.6359 - accuracy: 0.1417 - val_loss: 10.2060 - val_accuracy: 0.1221\nEpoch 10/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.6768 - accuracy: 0.1600 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 11/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 12/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 13/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 14/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 15/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 16/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 17/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 18/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 19/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 20/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 21/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 22/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 23/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 24/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 25/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 26/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 27/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 28/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 29/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 30/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 31/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 32/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 33/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 34/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 35/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 36/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 37/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 38/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 39/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 40/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 41/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 42/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 43/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 44/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 45/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 46/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 47/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 48/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 49/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 50/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 51/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 52/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 53/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 54/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 55/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 56/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\n","name":"stdout"},{"output_type":"stream","text":"Epoch 57/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 58/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 59/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 60/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 61/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 62/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 63/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 64/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 65/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 66/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 67/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 68/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 69/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 70/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 71/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 72/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 73/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 74/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 75/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 76/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 77/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 78/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 79/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 80/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 81/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 82/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 83/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 84/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 85/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 86/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 87/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 88/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 89/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 90/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 91/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 92/100\n1875/1875 [==============================] - 6s 3ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 93/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 94/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 95/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 96/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 97/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9964 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 98/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 99/100\n1875/1875 [==============================] - 4s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\nEpoch 100/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 9.9965 - accuracy: 0.2038 - val_loss: 10.0094 - val_accuracy: 0.2039\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf6436a850>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Doesn't seem to help\n\nSo we try to let the model to label the first 20 images instead and should overfit"},{"metadata":{"trusted":true},"cell_type":"code","source":"run = wandb.init(project=\"fashion\")\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\nmodel.add(Dense(num_classes))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.fit(X_train[:20, :, :], y_train[:20, :], epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/2nlc10rr\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/2nlc10rr</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1/1 [==============================] - 1s 843ms/step - loss: 8.9311 - accuracy: 0.2000 - val_loss: 7.0201 - val_accuracy: 0.0556\nEpoch 2/10\n1/1 [==============================] - 1s 686ms/step - loss: 9.3627 - accuracy: 0.2000 - val_loss: 6.5489 - val_accuracy: 0.0654\nEpoch 3/10\n1/1 [==============================] - 1s 895ms/step - loss: 5.8734 - accuracy: 0.2500 - val_loss: 7.6118 - val_accuracy: 0.0771\nEpoch 4/10\n1/1 [==============================] - 1s 679ms/step - loss: 7.4620 - accuracy: 0.2500 - val_loss: 8.4927 - val_accuracy: 0.0875\nEpoch 5/10\n1/1 [==============================] - 1s 684ms/step - loss: 8.1713 - accuracy: 0.2000 - val_loss: 8.9698 - val_accuracy: 0.0972\nEpoch 6/10\n1/1 [==============================] - 1s 661ms/step - loss: 8.0714 - accuracy: 0.2000 - val_loss: 9.5013 - val_accuracy: 0.1019\nEpoch 7/10\n1/1 [==============================] - 1s 702ms/step - loss: 9.6748 - accuracy: 0.2000 - val_loss: 9.8614 - val_accuracy: 0.1072\nEpoch 8/10\n1/1 [==============================] - 1s 673ms/step - loss: 11.2827 - accuracy: 0.2000 - val_loss: 10.1312 - val_accuracy: 0.1120\nEpoch 9/10\n1/1 [==============================] - 1s 747ms/step - loss: 13.7004 - accuracy: 0.2000 - val_loss: 10.2737 - val_accuracy: 0.1166\nEpoch 10/10\n1/1 [==============================] - 1s 819ms/step - loss: 14.5063 - accuracy: 0.2500 - val_loss: 10.3559 - val_accuracy: 0.1240\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf6411d190>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The result is still bad"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(X_test[:10,:,:]))","execution_count":13,"outputs":[{"output_type":"stream","text":"[[-319.75983   -494.3503    -370.06165     92.437645   -25.404772\n   104.11589    -30.54483    170.83455    -24.021126   188.16199  ]\n [-705.3608    -748.034     -632.92926    261.6067     282.58383\n   416.62024    403.49185     10.543699    85.329216   715.23975  ]\n [-357.10022   -141.94008   -470.60654     84.49116    145.18173\n   497.71707     28.790054    68.76871   -118.483315   329.6385   ]\n [-192.21545    -59.665817  -292.73697     64.67023    177.2555\n   286.2984      -8.011223    24.36292      4.7557263  179.272    ]\n [-544.3629    -485.1159    -396.63373    133.5525     118.07027\n   272.4465     262.49628    -82.15984     31.534428   374.317    ]\n [-393.06372   -268.1349    -370.1016     125.991844   184.20653\n   398.89218    216.65108     91.902954   -77.849945   360.3872   ]\n [-306.84003   -361.35046   -119.46768    146.63646    216.81999\n   199.76332    198.73637    152.82129   -119.84791    160.901    ]\n [-365.01273   -294.2553    -173.37085     63.520878   199.0384\n   225.1558     221.16443    -12.59786    -90.04829    294.85382  ]\n [ -45.54533   -113.22736     -9.890019   -46.67424     22.646368\n     7.707953    14.092191    80.224686   -10.889862    55.468384 ]\n [-139.07721   -265.9018    -264.4698      -2.5147946   18.001616\n    82.361885    -2.3578641   51.699417    15.007213   110.54707  ]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We expect numbers between 0 and 1 but we're getting huge positive and negative numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"run = wandb.init(project=\"fashion\")\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\n# added sigmoid activation\nmodel.add(Dense(num_classes, activation=\"sigmoid\"))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/2cd9m3ln\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/2cd9m3ln</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.0108 - accuracy: 0.1267 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 3/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 6/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\nEpoch 10/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1921e-07 - accuracy: 0.1000 - val_loss: 1.1921e-07 - val_accuracy: 0.1000\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf56e83950>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(X_test[:10,:,:]))","execution_count":16,"outputs":[{"output_type":"stream","text":"[[1.5394289e-05 8.6471982e-06 1.6164071e-04 5.6564822e-05 1.8595402e-04\n  1.2615754e-01 1.2612614e-04 1.9971715e-01 2.1010131e-02 6.5256095e-01]\n [1.4471510e-04 2.2127340e-06 9.4367868e-01 5.6870762e-05 5.2319230e-03\n  6.6265535e-08 5.0842669e-02 1.2351475e-10 4.2831056e-05 6.5066539e-09]\n [8.6950895e-05 9.9974185e-01 1.8133162e-05 8.0544531e-05 7.0497765e-05\n  2.3896584e-11 1.4728279e-06 8.4228965e-08 3.0777960e-07 1.1525889e-08]\n [3.1875261e-05 9.9895179e-01 8.7533968e-05 8.1032974e-04 1.0959970e-04\n  1.4894068e-09 7.1831087e-06 2.9057594e-07 3.9752717e-07 8.8981807e-07]\n [2.3257497e-01 1.5930808e-03 1.5462482e-01 5.2585836e-02 2.7411230e-02\n  1.9055710e-04 5.2061725e-01 4.4754920e-06 1.0303198e-02 9.4702038e-05]\n [9.2929760e-03 9.8131865e-01 6.3307805e-04 1.6009803e-03 6.9291717e-03\n  1.0387462e-09 2.1294625e-04 5.6298069e-07 1.1441280e-05 2.8186173e-07]\n [2.7847223e-02 9.4545534e-04 1.6389791e-02 1.7498849e-03 8.9071655e-01\n  3.4399095e-04 5.8052164e-02 2.1251855e-05 3.8918939e-03 4.1847528e-05]\n [4.3197125e-03 4.5613688e-03 6.2029637e-02 5.3812936e-03 2.9394576e-01\n  2.2488504e-04 6.1900222e-01 5.5530104e-06 1.0455119e-02 7.4436757e-05]\n [1.1442329e-02 2.9597482e-02 3.4786042e-02 3.7384417e-02 2.2746669e-02\n  4.1083768e-01 2.6686350e-02 3.1842273e-01 9.2061125e-02 1.6035125e-02]\n [2.4439840e-05 3.4176719e-05 9.4925643e-05 9.5132818e-05 5.1250754e-05\n  3.0032428e-02 9.1408881e-05 9.6193928e-01 4.0525175e-03 3.5844245e-03]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\n\n# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nlabels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\n          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nnum_classes = y_train.shape[1]\n\nrun = wandb.init(project=\"fashion\")\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\n# added sigmoid activation\nmodel.add(Dense(num_classes, activation=\"softmax\"))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/3ghi2d9w\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/3ghi2d9w</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 16.7667 - accuracy: 0.7466 - val_loss: 12.9043 - val_accuracy: 0.7811\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 11.8407 - accuracy: 0.7896 - val_loss: 12.1505 - val_accuracy: 0.7778\nEpoch 3/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 11.3901 - accuracy: 0.7949 - val_loss: 17.2371 - val_accuracy: 0.7621\nEpoch 4/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 10.9832 - accuracy: 0.7987 - val_loss: 10.4054 - val_accuracy: 0.8084\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.6116 - accuracy: 0.8011 - val_loss: 11.9373 - val_accuracy: 0.7816\nEpoch 6/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 10.4648 - accuracy: 0.8030 - val_loss: 11.9477 - val_accuracy: 0.7855\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 10.0341 - accuracy: 0.8058 - val_loss: 11.4912 - val_accuracy: 0.7714\nEpoch 8/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 10.1088 - accuracy: 0.8055 - val_loss: 13.7834 - val_accuracy: 0.7696\nEpoch 9/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 9.7283 - accuracy: 0.8095 - val_loss: 11.5831 - val_accuracy: 0.7871\nEpoch 10/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 10.0467 - accuracy: 0.8079 - val_loss: 11.2868 - val_accuracy: 0.7962\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf401fa890>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(X_test[:10,:,:]))","execution_count":22,"outputs":[{"output_type":"stream","text":"[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n  4.0290146e-27 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n  0.0000000e+00 1.4145855e-29 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [9.1110454e-18 0.0000000e+00 2.1881208e-02 0.0000000e+00 1.4930466e-23\n  0.0000000e+00 9.7811878e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n  0.0000000e+00 2.7526008e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.8896140e-28\n  0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [0.0000000e+00 2.4281667e-26 1.2322271e-32 0.0000000e+00 0.0000000e+00\n  1.0000000e+00 0.0000000e+00 3.1794808e-10 3.1071618e-21 0.0000000e+00]\n [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n  0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                7850      \n=================================================================\nTotal params: 7,850\nTrainable params: 7,850\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Multi-Layer Perceptrons"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\n\n# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nlabels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\n          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nnum_classes = y_train.shape[1]\n\nrun = wandb.init(project=\"fashion\")\n\n# Normalizing the data\nX_train = X_train / 255.\nX_test = X_test / 255.\n\nnum_classes = y_train.shape[1]\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\n# Adding activation function to make sure the output is between 0 - 1\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n# Loss function changed to categorical_crossentropy\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/ckv8ifc2\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/ckv8ifc2</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5920 - accuracy: 0.8012 - val_loss: 0.5071 - val_accuracy: 0.8260\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4609 - accuracy: 0.8418 - val_loss: 0.4775 - val_accuracy: 0.8329\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4357 - accuracy: 0.8513 - val_loss: 0.4963 - val_accuracy: 0.8188\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4229 - accuracy: 0.8542 - val_loss: 0.4744 - val_accuracy: 0.8351\nEpoch 5/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4124 - accuracy: 0.8570 - val_loss: 0.4465 - val_accuracy: 0.8436\nEpoch 6/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4066 - accuracy: 0.8585 - val_loss: 0.4506 - val_accuracy: 0.8426\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4021 - accuracy: 0.8601 - val_loss: 0.4534 - val_accuracy: 0.8435\nEpoch 8/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3977 - accuracy: 0.8618 - val_loss: 0.4643 - val_accuracy: 0.8400\nEpoch 9/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3955 - accuracy: 0.8626 - val_loss: 0.4608 - val_accuracy: 0.8391\nEpoch 10/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3912 - accuracy: 0.8641 - val_loss: 0.4468 - val_accuracy: 0.8435\n","name":"stdout"},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf400d1290>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\n\n# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nlabels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\n          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nnum_classes = y_train.shape[1]\n\nrun = wandb.init(project=\"fashion\")\n\n# Normalizing the data\nX_train = X_train / 255.\nX_test = X_test / 255.\n\nnum_classes = y_train.shape[1]\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\n# Add additional hidden layer\nmodel.add(Dense(100, activation=\"relu\"))\n# Adding activation function to make sure the output is between 0 - 1\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n# Loss function changed to categorical_crossentropy\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/2rwboegu\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/2rwboegu</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5049 - accuracy: 0.8245 - val_loss: 0.4541 - val_accuracy: 0.8364\nEpoch 2/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3764 - accuracy: 0.8645 - val_loss: 0.3823 - val_accuracy: 0.8655\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3384 - accuracy: 0.8770 - val_loss: 0.3694 - val_accuracy: 0.8683\nEpoch 4/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3161 - accuracy: 0.8849 - val_loss: 0.3692 - val_accuracy: 0.8655\nEpoch 5/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2987 - accuracy: 0.8894 - val_loss: 0.3745 - val_accuracy: 0.8699\nEpoch 6/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.2863 - accuracy: 0.8945 - val_loss: 0.3417 - val_accuracy: 0.8792\nEpoch 7/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2727 - accuracy: 0.8989 - val_loss: 0.3593 - val_accuracy: 0.8779\nEpoch 8/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.2622 - accuracy: 0.9025 - val_loss: 0.3421 - val_accuracy: 0.8820\nEpoch 9/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.2526 - accuracy: 0.9060 - val_loss: 0.3337 - val_accuracy: 0.8840\nEpoch 10/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2449 - accuracy: 0.9098 - val_loss: 0.3454 - val_accuracy: 0.8819\n","name":"stdout"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf56ac9610>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Add Dropout to reduce overfitting in between the perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\n\n# one hot encode outputs\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nlabels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\n          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nnum_classes = y_train.shape[1]\n\nrun = wandb.init(project=\"fashion\")\n\n# Normalizing the data\nX_train = X_train / 255.\nX_test = X_test / 255.\n\nnum_classes = y_train.shape[1]\n\n# create model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(img_width, img_height)))\n# Add Dropout\nmodel.add(Dropout(0.4))\n# Add additional hidden layer\nmodel.add(Dense(100, activation=\"relu\"))\nmodel.add(Dropout(0.4))\n# Adding activation function to make sure the output is between 0 - 1\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n# Loss function changed to categorical_crossentropy\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n          callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/jremeh/fashion\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/jremeh/fashion/runs/2uboarq4\" target=\"_blank\">https://app.wandb.ai/jremeh/fashion/runs/2uboarq4</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6899 - accuracy: 0.7491 - val_loss: 0.4755 - val_accuracy: 0.8287\nEpoch 2/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5358 - accuracy: 0.8030 - val_loss: 0.4426 - val_accuracy: 0.8354\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5013 - accuracy: 0.8162 - val_loss: 0.4261 - val_accuracy: 0.8439\nEpoch 4/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4842 - accuracy: 0.8207 - val_loss: 0.4138 - val_accuracy: 0.8461\nEpoch 5/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4729 - accuracy: 0.8259 - val_loss: 0.4035 - val_accuracy: 0.8550\nEpoch 6/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4674 - accuracy: 0.8282 - val_loss: 0.4035 - val_accuracy: 0.8528\nEpoch 7/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4584 - accuracy: 0.8303 - val_loss: 0.3909 - val_accuracy: 0.8617\nEpoch 8/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4559 - accuracy: 0.8309 - val_loss: 0.3878 - val_accuracy: 0.8615\nEpoch 9/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4496 - accuracy: 0.8334 - val_loss: 0.3954 - val_accuracy: 0.8560\nEpoch 10/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4466 - accuracy: 0.8342 - val_loss: 0.3896 - val_accuracy: 0.8584\n","name":"stdout"},{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7faf4007efd0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}